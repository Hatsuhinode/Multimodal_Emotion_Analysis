{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5fe0b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T19:16:16.081616Z",
     "iopub.status.busy": "2025-06-13T19:16:16.081341Z",
     "iopub.status.idle": "2025-06-13T19:18:30.837739Z",
     "shell.execute_reply": "2025-06-13T19:18:30.837099Z"
    },
    "papermill": {
     "duration": 134.760793,
     "end_time": "2025-06-13T19:18:30.839156",
     "exception": false,
     "start_time": "2025-06-13T19:16:16.078363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# IMPORTS AND SETUP\n",
    "# =====================================\n",
    "\n",
    "!pip install torch torchvision pytorchvideo > /dev/null 2>&1\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from pytorchvideo.models.hub import slowfast_r50\n",
    "import torchvision.transforms as transforms\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import random\n",
    "from huggingface_hub import HfApi, hf_hub_download, create_repo\n",
    "import torch.nn as nn\n",
    "from typing import Dict, Any, Optional\n",
    "from torchvision import transforms\n",
    "import pytorchvideo.models.hub as models\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import shutil\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import re\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2  # Fixed import\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42eb1743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T19:18:30.843982Z",
     "iopub.status.busy": "2025-06-13T19:18:30.843628Z",
     "iopub.status.idle": "2025-06-13T19:18:30.847766Z",
     "shell.execute_reply": "2025-06-13T19:18:30.847204Z"
    },
    "papermill": {
     "duration": 0.00758,
     "end_time": "2025-06-13T19:18:30.848832",
     "exception": false,
     "start_time": "2025-06-13T19:18:30.841252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reuse your existing configurations and dataset class\n",
    "class Config:\n",
    "    num_frames = 32\n",
    "    crop_size = (224, 224)\n",
    "    dataset_mean = [0.45, 0.45, 0.45]\n",
    "    dataset_std = [0.225, 0.225, 0.225]\n",
    "    batch_size = 16\n",
    "    resize_size = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397996df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T19:18:30.853186Z",
     "iopub.status.busy": "2025-06-13T19:18:30.852979Z",
     "iopub.status.idle": "2025-06-13T19:18:30.864075Z",
     "shell.execute_reply": "2025-06-13T19:18:30.863603Z"
    },
    "papermill": {
     "duration": 0.014546,
     "end_time": "2025-06-13T19:18:30.865112",
     "exception": false,
     "start_time": "2025-06-13T19:18:30.850566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# DATASET CLASS\n",
    "# =====================================\n",
    "\n",
    "class MELDDataset(Dataset):\n",
    "    \"\"\"Enhanced Dataset with Temporal Augmentations\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata, split, train=True):\n",
    "        self.data = metadata[split]\n",
    "        self.train = train\n",
    "        self.transform = self._build_transforms()\n",
    "        self.error_log = open(\"dataset_errors.log\", \"a\")\n",
    "\n",
    "    def _build_transforms(self):\n",
    "        \"\"\"Build data augmentation transforms\"\"\"\n",
    "        normalize = A.Normalize(\n",
    "            mean=Config.dataset_mean,\n",
    "            std=Config.dataset_std,\n",
    "            max_pixel_value=255.0\n",
    "        )\n",
    "        \n",
    "        if self.train:\n",
    "            return A.Compose([\n",
    "                A.Resize(Config.resize_size[0], Config.resize_size[1]),\n",
    "                A.RandomCrop(height=Config.crop_size[0], width=Config.crop_size[1], p=1.0),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.Rotate(limit=15, p=0.4),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                normalize,\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            return A.Compose([\n",
    "                A.Resize(Config.resize_size[0], Config.resize_size[1]),\n",
    "                A.CenterCrop(height=Config.crop_size[0], width=Config.crop_size[1], p=1.0),\n",
    "                normalize,\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        frames_dir = item['frames_dir']\n",
    "        mask_info = item['mask_info']\n",
    "        label = item['y']\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(frames_dir):\n",
    "                raise FileNotFoundError(f\"Directory not found: {frames_dir}\")\n",
    "            \n",
    "            frame_files = sorted(\n",
    "                [f for f in os.listdir(frames_dir) if f.endswith(('.jpg', '.png'))],\n",
    "                key=lambda x: int(re.search(r'^(\\d+)', x).group(1))\n",
    "            )\n",
    "            \n",
    "            if len(frame_files) < Config.num_frames:\n",
    "                raise ValueError(f\"Only {len(frame_files)} frames found, need {Config.num_frames}\")\n",
    "\n",
    "            # FIXED MASK HANDLING\n",
    "            if len(frame_files) > Config.num_frames:\n",
    "                start_idx = random.randint(0, len(frame_files) - Config.num_frames)\n",
    "                selected_files = frame_files[start_idx:start_idx+Config.num_frames]\n",
    "                selected_mask_info = mask_info[start_idx:start_idx+Config.num_frames]\n",
    "            else:\n",
    "                selected_files = frame_files\n",
    "                selected_mask_info = mask_info[:len(selected_files)]\n",
    "                if len(selected_mask_info) < Config.num_frames:\n",
    "                    pad_length = Config.num_frames - len(selected_mask_info)\n",
    "                    selected_mask_info = selected_mask_info + [0] * pad_length\n",
    "\n",
    "            frames = []\n",
    "            for i, fname in enumerate(selected_files):\n",
    "                frame_path = os.path.join(frames_dir, fname)\n",
    "                frame = cv2.imread(frame_path)\n",
    "                if frame is None:\n",
    "                    raise IOError(f\"Failed to read {frame_path}\")\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                transformed = self.transform(image=frame)[\"image\"]\n",
    "                frames.append(transformed)\n",
    "\n",
    "            video_tensor = torch.stack(frames)  # [T, C, H, W]\n",
    "            slow_pathway = video_tensor[::4].permute(1, 0, 2, 3)  # [C, T/4, H, W]\n",
    "            fast_pathway = video_tensor.permute(1, 0, 2, 3)       # [C, T, H, W]\n",
    "            \n",
    "            mask = torch.tensor(selected_mask_info[:Config.num_frames], dtype=torch.float32)\n",
    "            slow_mask = mask[::4]\n",
    "            fast_mask = mask\n",
    "            \n",
    "            return slow_pathway, fast_pathway, slow_mask, fast_mask, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.error_log.write(f\"Error loading index {idx}: {str(e)}\\n\")\n",
    "            slow = torch.zeros(3, Config.num_frames // 4, *Config.crop_size)\n",
    "            fast = torch.zeros(3, Config.num_frames, *Config.crop_size)\n",
    "            slow_mask = torch.ones(Config.num_frames // 4)\n",
    "            fast_mask = torch.ones(Config.num_frames)\n",
    "            return slow, fast, slow_mask, fast_mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.error_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b64f99d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T19:18:30.869251Z",
     "iopub.status.busy": "2025-06-13T19:18:30.869044Z",
     "iopub.status.idle": "2025-06-13T19:18:30.876155Z",
     "shell.execute_reply": "2025-06-13T19:18:30.875641Z"
    },
    "papermill": {
     "duration": 0.010358,
     "end_time": "2025-06-13T19:18:30.877159",
     "exception": false,
     "start_time": "2025-06-13T19:18:30.866801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# MODEL ARCHITECTURE\n",
    "# =====================================\n",
    "\n",
    "class MaskedSlowFast(nn.Module):\n",
    "    \"\"\"Simplified Model Architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = slowfast_r50(pretrained=True, progress=True)\n",
    "        self.backbone.blocks = self.backbone.blocks[:-1]  # Remove classification head\n",
    "\n",
    "        # Add batch normalization before classifier\n",
    "        self.feature_bn = nn.BatchNorm1d(2304)\n",
    "        \n",
    "        # Simplified classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2304, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Freeze initial layers\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Gradual unfreezing setup\n",
    "        self.unfreeze_stages = {5: False, 4: False, 3: False}\n",
    "\n",
    "    def unfreeze_layers(self, epoch):\n",
    "        \"\"\"Gradual layer unfreezing during training\"\"\"\n",
    "        if epoch >= 3 and not self.unfreeze_stages[5]:\n",
    "            self._unfreeze_stage(5)\n",
    "        if epoch >= 6 and not self.unfreeze_stages[4]:\n",
    "            self._unfreeze_stage(4)\n",
    "        if epoch >= 9 and not self.unfreeze_stages[3]:\n",
    "            self._unfreeze_stage(3)\n",
    "            \n",
    "    def _unfreeze_stage(self, stage):\n",
    "        \"\"\"Unfreeze a specific stage of the backbone\"\"\"\n",
    "        for param in self.backbone.blocks[stage].parameters():\n",
    "            param.requires_grad = True\n",
    "        self.unfreeze_stages[stage] = True\n",
    "        print(f\"Unfroze stage {stage} layers\")\n",
    "\n",
    "    def forward(self, slow_input, fast_input, slow_mask, fast_mask):\n",
    "        # Apply masks to zero out padded frames\n",
    "        slow_input = slow_input * slow_mask[:, None, :, None, None]\n",
    "        fast_input = fast_input * fast_mask[:, None, :, None, None]\n",
    "        \n",
    "        # Get features\n",
    "        features = self.backbone([slow_input, fast_input])\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        # Apply feature batch normalization\n",
    "        features = self.feature_bn(features)\n",
    "        \n",
    "        return self.classifier(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67970c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T19:18:30.881502Z",
     "iopub.status.busy": "2025-06-13T19:18:30.881029Z",
     "iopub.status.idle": "2025-06-13T19:18:30.885565Z",
     "shell.execute_reply": "2025-06-13T19:18:30.884940Z"
    },
    "papermill": {
     "duration": 0.007698,
     "end_time": "2025-06-13T19:18:30.886557",
     "exception": false,
     "start_time": "2025-06-13T19:18:30.878859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extended dataset to return video IDs\n",
    "class FeatureExtractionDataset(MELDDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        # Find video ID key (non-standard keys)\n",
    "        standard_keys = {'y', 'label', 'frames_dir', 'mask_info'}\n",
    "        video_id = next((k for k in item.keys() if k not in standard_keys), None)\n",
    "        \n",
    "        # Get original data\n",
    "        slow, fast, slow_mask, fast_mask, label = super().__getitem__(idx)\n",
    "        return slow, fast, slow_mask, fast_mask, label, video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71cf3a20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T19:18:30.890781Z",
     "iopub.status.busy": "2025-06-13T19:18:30.890605Z",
     "iopub.status.idle": "2025-06-13T19:18:30.898126Z",
     "shell.execute_reply": "2025-06-13T19:18:30.897654Z"
    },
    "papermill": {
     "duration": 0.010877,
     "end_time": "2025-06-13T19:18:30.899043",
     "exception": false,
     "start_time": "2025-06-13T19:18:30.888166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features():\n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(\"/kaggle/input/meld-extracted-video-frames-rgb/extraction_checkpoint.json\") as f:\n",
    "        metadata = json.load(f)[\"metadata\"]\n",
    "    \n",
    "    # Load model\n",
    "    num_classes = 7  # Adjust based on your dataset\n",
    "    model = MaskedSlowFast(num_classes).to(device)\n",
    "    \n",
    "    # Download model checkpoint from Hugging Face Hub\n",
    "    user_secrets = UserSecretsClient()\n",
    "    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "    checkpoint_path = hf_hub_download(\n",
    "        repo_id=\"prakanda/hatsu-meld-emotion-recognition-new\",\n",
    "        filename=\"best_model.pth\",\n",
    "        token=hf_token\n",
    "    )\n",
    "    \n",
    "    # Load model weights and remove classifier\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.classifier = torch.nn.Identity()  # Return features instead of predictions\n",
    "    model.eval()\n",
    "    \n",
    "    # Create datasets with video IDs\n",
    "    splits = ['train', 'dev', 'test']\n",
    "    datasets = {\n",
    "        split: FeatureExtractionDataset(metadata, split, train=False)\n",
    "        for split in splits\n",
    "    }\n",
    "    \n",
    "    # Create data loaders\n",
    "    loaders = {\n",
    "        split: DataLoader(\n",
    "            dataset, \n",
    "            batch_size=Config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    }\n",
    "    \n",
    "    # Extract features\n",
    "    features_dict = {}\n",
    "    for split, loader in loaders.items():\n",
    "        for batch in tqdm(loader, desc=f\"Extracting {split} features\"):\n",
    "            slow, fast, slow_mask, fast_mask, labels, video_ids = batch\n",
    "            slow = slow.to(device)\n",
    "            fast = fast.to(device)\n",
    "            slow_mask = slow_mask.to(device)\n",
    "            fast_mask = fast_mask.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                features = model(slow, fast, slow_mask, fast_mask)\n",
    "                features = features.cpu().numpy()\n",
    "            \n",
    "            for i, vid in enumerate(video_ids):\n",
    "                features_dict[vid] = features[i].tolist()\n",
    "    \n",
    "    # Add features to metadata\n",
    "    standard_keys = {'y', 'label', 'frames_dir', 'mask_info'}\n",
    "    for split in splits:\n",
    "        for item in metadata[split]:\n",
    "            # Find video ID key\n",
    "            video_id = next((k for k in item.keys() if k not in standard_keys), None)\n",
    "            if video_id and video_id in features_dict:\n",
    "                feature_key = f\"{video_id}__slowfast_features\"\n",
    "                item[feature_key] = features_dict[video_id]\n",
    "    \n",
    "    # Save results\n",
    "    output = metadata\n",
    "    with open(\"Video_Features_SlowFast.json\", \"w\") as f:\n",
    "        json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb0c8e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T19:18:30.903140Z",
     "iopub.status.busy": "2025-06-13T19:18:30.902949Z",
     "iopub.status.idle": "2025-06-13T19:39:54.946029Z",
     "shell.execute_reply": "2025-06-13T19:39:54.944659Z"
    },
    "papermill": {
     "duration": 1284.046752,
     "end_time": "2025-06-13T19:39:54.947531",
     "exception": false,
     "start_time": "2025-06-13T19:18:30.900779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_8x8_R50.pyth\" to /root/.cache/torch/hub/checkpoints/SLOWFAST_8x8_R50.pyth\n",
      "100%|██████████| 264M/264M [00:01<00:00, 245MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478127c1f01246b3b3a8e252b7912814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "best_model.pth:   0%|          | 0.00/426M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features: 100%|██████████| 625/625 [14:50<00:00,  1.42s/it]\n",
      "Extracting dev features: 100%|██████████| 70/70 [01:40<00:00,  1.44s/it]\n",
      "Extracting test features: 100%|██████████| 164/164 [04:08<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extract_features()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7523886,
     "sourceId": 12004167,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1425.841512,
   "end_time": "2025-06-13T19:39:57.698461",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-13T19:16:11.856949",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "025a7385ee0049e1b47cdc0d1045ca3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "054bd80b01154482bf47139d3df7158b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2a2dd4e6f5154524ad4b316f928713ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "34ca1c3109f544bc8c3d36ae7d0890c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6f1edcd2c71848008a60aa3bc32b7cc7",
       "placeholder": "​",
       "style": "IPY_MODEL_054bd80b01154482bf47139d3df7158b",
       "tabbable": null,
       "tooltip": null,
       "value": "best_model.pth: 100%"
      }
     },
     "478127c1f01246b3b3a8e252b7912814": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_34ca1c3109f544bc8c3d36ae7d0890c2",
        "IPY_MODEL_d743214140c84402b653f78969b0ecb4",
        "IPY_MODEL_b71438c872834c8ab1b772feb1f25832"
       ],
       "layout": "IPY_MODEL_742874bb75d34747a997b68f4edfaaea",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6f1edcd2c71848008a60aa3bc32b7cc7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "742874bb75d34747a997b68f4edfaaea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8fe2501e81be40509fc8b3c790f4bc90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aff7bcd8e64d407f85dd913c1fb4b3f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b71438c872834c8ab1b772feb1f25832": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_aff7bcd8e64d407f85dd913c1fb4b3f1",
       "placeholder": "​",
       "style": "IPY_MODEL_8fe2501e81be40509fc8b3c790f4bc90",
       "tabbable": null,
       "tooltip": null,
       "value": " 426M/426M [00:05&lt;00:00, 82.9MB/s]"
      }
     },
     "d743214140c84402b653f78969b0ecb4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_025a7385ee0049e1b47cdc0d1045ca3c",
       "max": 426423269.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2a2dd4e6f5154524ad4b316f928713ff",
       "tabbable": null,
       "tooltip": null,
       "value": 426423269.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
